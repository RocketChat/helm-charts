## Common scheduling configuration across all deployments
## Set these values once and they apply to all components via YAML anchors
## 
## Example with tolerations:
##   scheduling:
##     tolerations:
##       - key: "dedicated"
##         operator: "Equal"
##         value: "rocketchat"
##         effect: "NoSchedule"
##
scheduling:
  tolerations: &tolerations []
  nodeSelector: &nodeSelector {}
  affinity: &affinity {}

## Global values passed to subcharts and used by custom components
global:
  tolerations: *tolerations
  annotations: {}
  nodeSelector: *nodeSelector
  affinity: *affinity

operator:
  enabled: true
  global:
    rbac:
      create: true

  prometheusOperator:
    enabled: true
    tolerations: *tolerations
    admissionWebhooks:
      patch:
        tolerations: *tolerations

  crds:
    enabled: true

  defaultRules:
    create: false

  windowsMonitoring:
    enabled: false

  alertmanager:
    enabled: false

  kubernetesServiceMonitors:
    enabled: true

  kubeApiServer:
    enabled: true

  kubelet:
    enabled: true

  kubeControllerManager:
    enabled: true

  coreDns:
    enabled: false

  kubeDns:
    enabled: true

  kubeEtcd:
    enabled: true

  kubeScheduler:
    enabled: true

  kubeProxy:
    enabled: true

  kubeStateMetrics:
    enabled: true

  kube-state-metrics:
    tolerations: *tolerations

  nodeExporter:
    enabled: true

  prometheus-node-exporter:
    tolerations: *tolerations

  cleanPrometheusOperatorObjectNames: false

  extraManifests: null

  grafana:
    enabled: false

  persistence:
    enabled: false
    # type: pvc
    # accessModes: ["ReadWriteOnce"]
    # size: 1Gi
    # finalizers:
    #   - kubernetes.io/pvc-protection

  prometheus:
    enabled: true
    prometheusSpec:
      enableAdminAPI: true
      # this allows to get monitors from any namespace
      serviceMonitorSelectorNilUsesHelmValues: false
      podMonitorSelectorNilUsesHelmValues: false
      retention: 15d
      retentionSize: 8GB
      scrapeInterval: 30s
      tolerations: *tolerations
      storageSpec:
        volumeClaimTemplate:
          spec:
            # Set if you want to use a specific storage class
            #storageClassName: "local-path"
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 10Gi
  ingress:
    enabled: false

ingress:
  enabled: false
  domain: "example.com" # Default domain for ingress
  ingressClassName: "" # nginx or traefik or blank for trying auto-detect
  tls: false
  prometheus:
    enabled: false
    host: ""
    path: ""
  grafana:
    enabled: true
    host: ""
    path: ""

grafana:
  enabled: true
  deploy:
    enabled: true
    # storageClassName: # Get available classes with: kubectl get storageclasses.
    nodePort: ""

  # Tolerations for grafana-operator and Grafana instance (uses scheduling anchors)
  tolerations: *tolerations
  nodeSelector: *nodeSelector
  affinity: *affinity
  folders:
    rocketchat:
      - name: "rocketchat"
        id: "rocketchat"
    extra: [] # If user wants to add extra folders, they can do so here
  dashboards:
    rocketchat:
      - name: "rocketchat-logs"
        folder: "rocketchat"
        id: "24736"
        revision: latest
      - name: "rocketchat-metrics"
        folder: "rocketchat"
        id: "23428"
        revision: latest
      - name: "rocketchat-microservices"
        folder: "rocketchat"
        id: "23427"
        revision: latest
      - name: "rocketchat-mongo-8"
        folder: "rocketchat"
        id: "24296"
        revision: latest
      - name: "rocketchat-mongo-7"
        folder: "rocketchat"
        id: "23712"
        revision: latest
    community:
      - name: "node-exporter-full"
        id: "1860"
        revision: latest
      - name: "kubernetes-deployment"
        id: "741"
        revision: latest
      - name: "kubernetes-pod"
        id: "740"
        revision: latest
      - name: "kubernetes-node"
        id: "739"
        revision: latest
      - name: "ingress-nginx"
        id: "20275"
        revision: latest
      - name: "ingress-traefik"
        id: "17347"
        revision: latest
    extra: [] # If user wants to add extra dashboards, they can do so here



loki:
  enabled: true

  monitoring:
    selfMonitoring:
      grafanaAgent:
        installOperator: false
  
  deploymentMode: SingleBinary

  singleBinary:
    replicas: 1
    tolerations: *tolerations
    persistence:
      # Storage allocation for logs. Ensure adequate space: daily_log_volume * retention_days * 1.2
      # If disk becomes full, Loki stops working and requires manual intervention
      size: 50Gi
    extraArgs:
      - -config.expand-env=true

  gateway:
    tolerations: *tolerations

  # Disable caches by default for smaller clusters (enables for production if needed)
  chunksCache:
    enabled: false
  resultsCache:
    enabled: false

  loki:
    auth_enabled: false
    commonConfig:
      replication_factor: 1
    storage:
      type: 'filesystem'
    schemaConfig:
      configs:
        - from: "2024-04-01"
          store: tsdb
          object_store: filesystem
          schema: v13
          index:
            prefix: index_
            period: 24h
    compactor:
      retention_enabled: true
      compaction_interval: 10m
      retention_delete_delay: 2h
      retention_delete_worker_count: 100
      delete_request_store: filesystem
    pattern_ingester:
        enabled: true
    limits_config:
      allow_structured_metadata: true
      volume_enabled: true
      # Log retention period. Older logs are automatically deleted.
      # WARNING: Increasing retention requires corresponding increase to singleBinary.persistence.size
      # If storage is exceeded, Loki will stop and manual intervention is needed
      retention_period: 15d
    distributor:
      otlp_config:
        default_resource_attributes_as_index_labels:
          # basic information
          - job
          - agent.name
          - agent.node
          # Cluster information
          - cluster.name
          # basic pod information
          - k8s.namespace.name
          - k8s.pod.name # k8s_pod
          - k8s.container.name # k8s_container
          - container.name
          # Deployment information
          - k8s.replicaset.name
          - k8s.deployment.name
          - k8s.statefulset.name
          - k8s.daemonset.name
          - k8s.cronjob.name
          - k8s.job.name


    ruler:
      enabled: true
      enable_api: true

        
  # since we are running as single-binary, disable everything else
  test:
    enabled: false
  lokiCanary:
    enabled: false
  minio:
    enabled: false
  rollout_operator:
    enabled: false
  backend:
    replicas: 0
  read:
    replicas: 0
  write:
    replicas: 0
  ingester:
    replicas: 0
  querier:
    replicas: 0
  queryFrontend:
    replicas: 0
  queryScheduler:
    replicas: 0
  distributor:
    replicas: 0
  compactor:
    replicas: 0
  indexGateway:
    replicas: 0
  bloomCompactor:
    replicas: 0
  bloomGateway:
    replicas: 0

opentelemetryCollector:
  mode: daemonset
  image:
    registry: otel/opentelemetry-collector-contrib
    tag: 0.143.0
    pullPolicy: IfNotPresent

  # Uses scheduling anchors from top of file
  tolerations: *tolerations
  nodeSelector: *nodeSelector
  affinity: *affinity

  logs:
    enabled: true
    resources:
      limits:
        cpu: 200m
        memory: 256Mi
      requests:
        cpu: 100m
        memory: 128Mi

  events:
    enabled: true
    resources:
      limits:
        cpu: 200m
        memory: 256Mi
      requests:
        cpu: 100m
        memory: 128Mi
